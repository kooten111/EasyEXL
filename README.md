# convert_quant_exl2
Convert FP16 models from .bin to safetensor (if necessary) and then quantize them with exllama2.

Run from exllama2/util directory
python convert-and-quant.py path/to/model

## Todo

Batch Jobs
