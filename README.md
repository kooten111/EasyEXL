# convert_quant_exl2
Convert FP16 models from .bin to safetensor and then quantize them with exllama2.
